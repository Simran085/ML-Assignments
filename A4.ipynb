{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3hTiRzkqxAgxQGoiAz/ps",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Simran085/ML_LAB_ASS_102216115/blob/main/A4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBnexJdwiGik",
        "outputId": "89edfcaf-5220-455c-e9eb-855e16244944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Learning Rate: 0.0001, Alpha: 1e-15, R2 Score: 0.9847\n",
            "Learning Rate: 0.0001, Alpha: 1e-10, R2 Score: 0.9847\n",
            "Learning Rate: 0.0001, Alpha: 1e-05, R2 Score: 0.9847\n",
            "Learning Rate: 0.0001, Alpha: 0.001, R2 Score: 0.9847\n",
            "Learning Rate: 0.0001, Alpha: 0, R2 Score: 0.9847\n",
            "Learning Rate: 0.0001, Alpha: 1, R2 Score: 0.9847\n",
            "Learning Rate: 0.0001, Alpha: 10, R2 Score: 0.9786\n",
            "Learning Rate: 0.0001, Alpha: 20, R2 Score: 0.9627\n",
            "Learning Rate: 0.001, Alpha: 1e-15, R2 Score: 0.9856\n",
            "Learning Rate: 0.001, Alpha: 1e-10, R2 Score: 0.9856\n",
            "Learning Rate: 0.001, Alpha: 1e-05, R2 Score: 0.9856\n",
            "Learning Rate: 0.001, Alpha: 0.001, R2 Score: 0.9856\n",
            "Learning Rate: 0.001, Alpha: 0, R2 Score: 0.9856\n",
            "Learning Rate: 0.001, Alpha: 1, R2 Score: 0.9849\n",
            "Learning Rate: 0.001, Alpha: 10, R2 Score: 0.9753\n",
            "Learning Rate: 0.001, Alpha: 20, R2 Score: 0.9546\n",
            "Learning Rate: 0.01, Alpha: 1e-15, R2 Score: -520.6752\n",
            "Learning Rate: 0.01, Alpha: 1e-10, R2 Score: -520.6752\n",
            "Learning Rate: 0.01, Alpha: 1e-05, R2 Score: -520.6752\n",
            "Learning Rate: 0.01, Alpha: 0.001, R2 Score: -520.6773\n",
            "Learning Rate: 0.01, Alpha: 0, R2 Score: -520.6752\n",
            "Learning Rate: 0.01, Alpha: 1, R2 Score: -522.7616\n",
            "Learning Rate: 0.01, Alpha: 10, R2 Score: -542.2993\n",
            "Learning Rate: 0.01, Alpha: 20, R2 Score: -565.6117\n",
            "Learning Rate: 0.1, Alpha: 1e-15, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 1e-10, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 1e-05, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 0.001, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 0, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 1, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 10, R2 Score: -131616.5130\n",
            "Learning Rate: 0.1, Alpha: 20, R2 Score: -131616.5130\n",
            "Learning Rate: 1, Alpha: 1e-15, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 1e-10, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 1e-05, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 0.001, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 0, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 1, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 10, R2 Score: -13038961.1940\n",
            "Learning Rate: 1, Alpha: 20, R2 Score: -13038961.1940\n",
            "Learning Rate: 10, Alpha: 1e-15, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 1e-10, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 1e-05, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 0.001, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 0, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 1, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 10, R2 Score: -1302671507.0268\n",
            "Learning Rate: 10, Alpha: 20, R2 Score: -1302671507.0268\n",
            "\n",
            "Best Parameters:\n",
            "Learning Rate: 0.001, Regularization Parameter (Alpha): 1e-15\n",
            "Best R2 Score: 0.9856\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Q1.\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Generate a dataset with highly correlated features\n",
        "np.random.seed(42)\n",
        "n_samples = 1000\n",
        "X_base = np.random.rand(n_samples, 1) * 10  # Base feature for correlation\n",
        "\n",
        "# Generate highly correlated columns\n",
        "X = np.hstack([X_base + np.random.normal(0, 0.1, (n_samples, 1)) * i for i in range(1, 8)])\n",
        "y = 3 * X_base[:, 0] + np.random.normal(0, 1, n_samples)  # Target variable with some noise\n",
        "\n",
        "# Convert to DataFrame\n",
        "df = pd.DataFrame(X, columns=[f'Feature_{i+1}' for i in range(X.shape[1])])\n",
        "df['Target'] = y\n",
        "\n",
        "# Step 2: Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 3: Implement Ridge Regression with Gradient Descent\n",
        "def ridge_regression_gradient_descent(X, y, alpha, learning_rate, n_iterations):\n",
        "    n_samples, n_features = X.shape\n",
        "    weights = np.zeros(n_features)\n",
        "    bias = 0\n",
        "    cost_history = []\n",
        "\n",
        "    for i in range(n_iterations):\n",
        "        # Prediction\n",
        "        y_pred = np.dot(X, weights) + bias\n",
        "\n",
        "        # Check for overflow or NaN\n",
        "        if np.any(np.isnan(y_pred)) or np.any(np.isinf(y_pred)):\n",
        "            print(f\"Warning: Numerical issues at iteration {i}. Check learning rate or alpha.\")\n",
        "            break\n",
        "\n",
        "        # Calculate gradients\n",
        "        dw = (-2 / n_samples) * np.dot(X.T, (y - y_pred)) + 2 * alpha * weights\n",
        "        db = (-2 / n_samples) * np.sum(y - y_pred)\n",
        "\n",
        "        # Gradient clipping (optional)\n",
        "        max_grad = 1e3  # Set a threshold for gradients\n",
        "        dw = np.clip(dw, -max_grad, max_grad)\n",
        "        db = np.clip(db, -max_grad, max_grad)\n",
        "\n",
        "        # Update weights and bias\n",
        "        weights -= learning_rate * dw\n",
        "        bias -= learning_rate * db\n",
        "\n",
        "        # Compute cost with regularization term\n",
        "        cost = (1 / n_samples) * np.sum((y - y_pred) ** 2) + alpha * np.sum(weights ** 2)\n",
        "        cost_history.append(cost)\n",
        "\n",
        "    return weights, bias, cost_history\n",
        "\n",
        "# Rerun the training loop with reasonable learning rates and alpha values\n",
        "\n",
        "\n",
        "# Rerun the training loop with reasonable learning rates and alpha values\n",
        "\n",
        "\n",
        "# Step 4: Test different values of learning rate and regularization parameter\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
        "regularization_params = [1e-15, 1e-10, 1e-5, 1e-3, 0, 1, 10, 20]\n",
        "\n",
        "best_r2_score = -np.inf\n",
        "best_params = (None, None)\n",
        "best_weights, best_bias = None, None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for alpha in regularization_params:\n",
        "        weights, bias, cost_history = ridge_regression_gradient_descent(X_train, y_train, alpha, lr, n_iterations=1000)\n",
        "        y_pred_train = np.dot(X_train, weights) + bias\n",
        "        y_pred_test = np.dot(X_test, weights) + bias\n",
        "\n",
        "        r2 = r2_score(y_test, y_pred_test)\n",
        "\n",
        "        if r2 > best_r2_score:\n",
        "            best_r2_score = r2\n",
        "            best_params = (lr, alpha)\n",
        "            best_weights, best_bias = weights, bias\n",
        "\n",
        "        print(f'Learning Rate: {lr}, Alpha: {alpha}, R2 Score: {r2:.4f}')\n",
        "\n",
        "# Step 5: Print the best parameters\n",
        "print(\"\\nBest Parameters:\")\n",
        "print(f'Learning Rate: {best_params[0]}, Regularization Parameter (Alpha): {best_params[1]}')\n",
        "print(f'Best R2 Score: {best_r2_score:.4f}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3.\n",
        "# Step 1: Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import RidgeCV, LassoCV\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Step 2: Load the California housing dataset\n",
        "housing = fetch_california_housing()\n",
        "X = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
        "y = housing.target\n",
        "\n",
        "# Step 3: Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Step 4: Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 5: Apply RidgeCV and LassoCV\n",
        "alphas = np.logspace(-6, 6, 13)  # Wide range of alpha values for cross-validation\n",
        "\n",
        "# RidgeCV\n",
        "ridge_cv = RidgeCV(alphas=alphas, scoring='r2', cv=5)\n",
        "ridge_cv.fit(X_train_scaled, y_train)\n",
        "y_pred_ridge = ridge_cv.predict(X_test_scaled)\n",
        "\n",
        "# LassoCV\n",
        "lasso_cv = LassoCV(alphas=alphas, cv=5, max_iter=10000)\n",
        "lasso_cv.fit(X_train_scaled, y_train)\n",
        "y_pred_lasso = lasso_cv.predict(X_test_scaled)\n",
        "\n",
        "# Step 6: Evaluate the models\n",
        "print(\"Ridge Regression\")\n",
        "print(f\"Best Alpha: {ridge_cv.alpha_}\")\n",
        "print(f\"R2 Score: {r2_score(y_test, y_pred_ridge):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_ridge):.4f}\")\n",
        "\n",
        "print(\"\\nLasso Regression\")\n",
        "print(f\"Best Alpha: {lasso_cv.alpha_}\")\n",
        "print(f\"R2 Score: {r2_score(y_test, y_pred_lasso):.4f}\")\n",
        "print(f\"Mean Squared Error: {mean_squared_error(y_test, y_pred_lasso):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6iguf0V3k2ya",
        "outputId": "cc13a378-cf91-4bbb-9512-8ab5c8366874"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ridge Regression\n",
            "Best Alpha: 1e-06\n",
            "R2 Score: 0.5758\n",
            "Mean Squared Error: 0.5559\n",
            "\n",
            "Lasso Regression\n",
            "Best Alpha: 0.001\n",
            "R2 Score: 0.5769\n",
            "Mean Squared Error: 0.5545\n"
          ]
        }
      ]
    }
  ]
}